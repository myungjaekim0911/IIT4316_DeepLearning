{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVpCD5fLZC6w"
   },
   "source": [
    "# IIT 4316 Deep Learning<br>Homework #1: Multi-Layered Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7NCYmnQqvPz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1RlRqQSrdGt"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi2Aw8RVqsyU"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ############################################################################\n",
    "    # TODO: Implement Sigmoid activation\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # END TODO\n",
    "    ############################################################################\n",
    "\n",
    "    raise NotImplementedError(\"sigmoid not implemented\")\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    ############################################################################\n",
    "    # TODO: Implement Sigmoid derivative\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # END TODO\n",
    "    ############################################################################\n",
    "\n",
    "    raise NotImplementedError(\"sigmoid_derivative not implemented\")\n",
    "\n",
    "def bce_loss(y_pred, y_true):\n",
    "    ############################################################################\n",
    "    # TODO: Implement Binary Cross Entropy Loss\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # END TODO\n",
    "    ############################################################################\n",
    "\n",
    "    raise NotImplementedError(\"bce_loss not implemented\")\n",
    "\n",
    "\n",
    "# ==================== Test Utility Functions ====================\n",
    "def test_utility_functions():\n",
    "    \"\"\"Test all utility functions with assertions\"\"\"\n",
    "    print(\"Testing utility functions...\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Test Sigmoid\n",
    "    x_test = torch.tensor([0.0])\n",
    "    sigmoid_out = sigmoid(x_test)\n",
    "    expected_sigmoid = torch.tensor([0.5])\n",
    "    assert torch.allclose(sigmoid_out, expected_sigmoid, atol=1e-6), f\"Sigmoid failed: expected {expected_sigmoid}, got {sigmoid_out}\"\n",
    "\n",
    "    x_test2 = torch.tensor([-100.0, 100.0])\n",
    "    sigmoid_out2 = sigmoid(x_test2)\n",
    "    assert sigmoid_out2[0] < 0.01 and sigmoid_out2[1] > 0.99, \"Sigmoid boundary values incorrect\"\n",
    "    print(\"✓ Sigmoid implementation correct\")\n",
    "\n",
    "    # Test Sigmoid derivative\n",
    "    x_test = torch.tensor([0.0])\n",
    "    sigmoid_deriv = sigmoid_derivative(x_test)\n",
    "    expected_sigmoid_deriv = torch.tensor([0.25])  # sigmoid(0) * (1 - sigmoid(0)) = 0.5 * 0.5\n",
    "    assert torch.allclose(sigmoid_deriv, expected_sigmoid_deriv, atol=1e-6), f\"Sigmoid derivative failed: expected {expected_sigmoid_deriv}, got {sigmoid_deriv}\"\n",
    "    print(\"✓ Sigmoid derivative implementation correct\")\n",
    "\n",
    "    # Test BCE Loss\n",
    "    y_pred = torch.tensor([[0.9], [0.1], [0.8], [0.2]])\n",
    "    y_true = torch.tensor([[1.0], [0.0], [1.0], [0.0]])\n",
    "    loss = bce_loss(y_pred, y_true)\n",
    "    assert loss > 0, \"BCE loss should be positive\"\n",
    "    assert not torch.isnan(loss), \"BCE loss contains NaN\"\n",
    "\n",
    "    # Perfect prediction should give near-zero loss\n",
    "    y_pred_perfect = torch.tensor([[0.9999], [0.0001], [0.9999], [0.0001]])\n",
    "    loss_perfect = bce_loss(y_pred_perfect, y_true)\n",
    "    assert loss_perfect < 0.01, f\"BCE loss for near-perfect prediction should be close to 0, got {loss_perfect}\"\n",
    "    print(\"✓ BCE Loss implementation correct\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"All utility functions passed!\\n\")\n",
    "\n",
    "test_utility_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGvi4p6k1Gvp"
   },
   "source": [
    "## MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1iw63kwrLjD"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, initial_weights=None):\n",
    "        if initial_weights is None:\n",
    "            self.W1 = torch.randn(2, 2, requires_grad=False)\n",
    "            self.B1 = torch.randn(2, requires_grad=False)\n",
    "            self.W2 = torch.randn(2, 1, requires_grad=False)\n",
    "            self.B2 = torch.randn(1, requires_grad=False)\n",
    "        else:\n",
    "            self.W1 = initial_weights['W1'].clone()\n",
    "            self.B1 = initial_weights['B1'].clone()\n",
    "            self.W2 = initial_weights['W2'].clone()\n",
    "            self.B2 = initial_weights['B2'].clone()\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self.X = None\n",
    "        self.Z1 = None\n",
    "        self.H1 = None\n",
    "        self.Z2 = None\n",
    "        self.Y_pred = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        Args:\n",
    "            X: (N, 2) input tensor\n",
    "\n",
    "        Returns:\n",
    "            Y_pred: (N, 1) output probabilities after sigmoid\n",
    "        \"\"\"\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Implement forward pass\n",
    "        #\n",
    "        # Save intermediate values for convenience & backward pass\n",
    "        #  - self.X: input \n",
    "        #  - self.Z1: input to the sigmoid in the hidden layer\n",
    "        #  - self.H1: output of the sigmoid in the hidden layer\n",
    "        #  - self.Z2: input to the sigmoid in the output layer\n",
    "        #  - self.Y_pred: output of the sigmoid in the output layer (=final output)\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "        # END TODO\n",
    "        ########################################################################\n",
    "\n",
    "        assert self.X is not None, \"X not saved during forward pass\"\n",
    "        assert self.Z1 is not None, \"Z1 not saved during forward pass\"\n",
    "        assert self.H1 is not None, \"H1 not saved during forward pass\"\n",
    "        assert self.Z2 is not None, \"Z2 not saved during forward pass\"\n",
    "        assert self.Y_pred is not None, \"Y_pred not saved during forward pass\"\n",
    "\n",
    "        raise NotImplementedError(\"forward not implemented\")\n",
    "\n",
    "    def backward(self, Y_true):\n",
    "        \"\"\"\n",
    "        Backward pass - compute gradients manually\n",
    "\n",
    "        Args:\n",
    "            Y_true: (N, 1) true labels\n",
    "\n",
    "        Returns:\n",
    "            gradients: dict with keys 'W1_12', 'W1_21'\n",
    "        \"\"\"\n",
    "        N = self.X.shape[0]\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Compute gradients manually using chain rule\n",
    "        #\n",
    "        # Goal: Compute the gradients of the BCE loss with respect to W1_12 and W1_21, stored as dL_dW1_12 and dL_dW1_21\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "        # END TODO\n",
    "        ########################################################################\n",
    "\n",
    "        assert dL_dW1_12 is not None, \"dL_dW1_12 not computed\"\n",
    "        assert dL_dW1_21 is not None, \"dL_dW1_21 not computed\"\n",
    "\n",
    "        return {\n",
    "            'W1_12': dL_dW1_12,\n",
    "            'W1_21': dL_dW1_21\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEep4DsyrY_X"
   },
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0ba0nfLrQEM"
   },
   "outputs": [],
   "source": [
    "def train(mlp, X, Y, optimizer='sgd', lr=0.1, epochs=1000, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"\n",
    "    Train the MLP using manual gradient computation\n",
    "\n",
    "    Args:\n",
    "        mlp: MLP instance\n",
    "        X: (N, 2) input data\n",
    "        Y: (N, 1) or (N,) true labels\n",
    "        optimizer: 'sgd' or 'adam'\n",
    "        lr: learning rate\n",
    "        epochs: number of training epochs\n",
    "        beta1: Adam parameter (momentum)\n",
    "        beta2: Adam parameter (RMSprop)\n",
    "\n",
    "    Returns:\n",
    "        history: dict with 'W1', 'B1', 'W2', 'B2', 'loss' lists\n",
    "    \"\"\"\n",
    "    # Ensure Y is (N, 1)\n",
    "    if Y.dim() == 1:\n",
    "        Y = Y.unsqueeze(1)\n",
    "\n",
    "    # Initialize history\n",
    "    history = {\n",
    "        'W1': [mlp.W1.clone()],\n",
    "        'B1': [mlp.B1.clone()],\n",
    "        'W2': [mlp.W2.clone()],\n",
    "        'B2': [mlp.B2.clone()],\n",
    "        'loss': []\n",
    "    }\n",
    "\n",
    "    # Initialize Adam parameters\n",
    "    if optimizer == 'adam':\n",
    "        m = {'W1_12': torch.tensor(0.0),\n",
    "             'W1_21': torch.tensor(0.0)}\n",
    "\n",
    "        v = {'W1_12': torch.tensor(0.0),\n",
    "             'W1_21': torch.tensor(0.0)}\n",
    "\n",
    "        epsilon = 1e-8\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        Y_pred = mlp.forward(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = bce_loss(Y_pred, Y)\n",
    "        history['loss'].append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = mlp.backward(Y)\n",
    "\n",
    "        # Update weights\n",
    "        if optimizer == 'sgd':\n",
    "            ####################################################################\n",
    "            # TODO: Implement SGD optimizer\n",
    "            #\n",
    "            # Assign new weights to W1_12, W1_21\n",
    "            ####################################################################\n",
    "\n",
    "\n",
    "\n",
    "            ####################################################################\n",
    "            # END TODO\n",
    "            ####################################################################\n",
    "\n",
    "            assert W1_12 is not None, \"W1_12 not computed\"\n",
    "            assert W1_21 is not None, \"W1_21 not computed\"\n",
    "\n",
    "            mlp.W1[0, 1] = W1_12\n",
    "            mlp.W1[1, 0] = W1_21\n",
    "\n",
    "        elif optimizer == 'adam':\n",
    "            ####################################################################\n",
    "            # TODO: Implement Adam optimizer\n",
    "            #\n",
    "            # Assign new weights to W1_12, W1_21\n",
    "            ####################################################################\n",
    "\n",
    "\n",
    "\n",
    "            ####################################################################\n",
    "            # END TODO\n",
    "            ####################################################################\n",
    "\n",
    "            assert W1_12 is not None, \"W1_12 not computed\"\n",
    "            assert W1_21 is not None, \"W1_21 not computed\"\n",
    "\n",
    "            mlp.W1[0, 1] = W1_12\n",
    "            mlp.W1[1, 0] = W1_21\n",
    "\n",
    "        # Store weights history\n",
    "        history['W1'].append(mlp.W1.clone())\n",
    "        history['B1'].append(mlp.B1.clone())\n",
    "        history['W2'].append(mlp.W2.clone())\n",
    "        history['B2'].append(mlp.B2.clone())\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79i6ZdqirauX"
   },
   "source": [
    "## Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4YbFybpZvbI"
   },
   "outputs": [],
   "source": [
    "def plot_loss_landscape(mlp_initial, trajectories, loss_trajectories, labels):\n",
    "\n",
    "    # Extract all W1[0,1] and W1[1,0] trajectories\n",
    "    all_w01_trajs = []\n",
    "    all_w10_trajs = []\n",
    "\n",
    "    for weight_trajectory in trajectories:\n",
    "        if isinstance(weight_trajectory[0], dict):\n",
    "            w01_traj = [w['W1'][0, 1].item() for w in weight_trajectory]\n",
    "            w10_traj = [w['W1'][1, 0].item() for w in weight_trajectory]\n",
    "        else:\n",
    "            w01_traj = [w[0][0, 1].item() for w in weight_trajectory]\n",
    "            w10_traj = [w[0][1, 0].item() for w in weight_trajectory]\n",
    "\n",
    "        all_w01_trajs.append(w01_traj)\n",
    "        all_w10_trajs.append(w10_traj)\n",
    "\n",
    "    # Use the first trajectory's final values as center\n",
    "    w01_final = all_w01_trajs[0][-1]\n",
    "    w10_final = all_w10_trajs[0][-1]\n",
    "\n",
    "    # Calculate grid limits considering all trajectories\n",
    "    w01_dist = max(abs(w - w01_final) for traj in all_w01_trajs for w in traj)\n",
    "    w10_dist = max(abs(w - w10_final) for traj in all_w10_trajs for w in traj)\n",
    "\n",
    "    w01_range = w01_dist * 1.2\n",
    "    w10_range = w10_dist * 1.2\n",
    "\n",
    "    w01_min, w01_max = w01_final - w01_range, w01_final + w01_range\n",
    "    w10_min, w10_max = w10_final - w10_range, w10_final + w10_range\n",
    "\n",
    "    # Create grid\n",
    "    w01_grid = np.linspace(w01_min, w01_max, 100)\n",
    "    w10_grid = np.linspace(w10_min, w10_max, 100)\n",
    "    W01_mesh, W10_mesh = np.meshgrid(w01_grid, w10_grid)\n",
    "\n",
    "    # Compute loss for each grid point\n",
    "    Loss_mesh = np.zeros_like(W01_mesh)\n",
    "\n",
    "    for i in range(W01_mesh.shape[0]):\n",
    "        for j in range(W01_mesh.shape[1]):\n",
    "            # Create temporary MLP with modified weights\n",
    "            temp_mlp = MLP()\n",
    "            # Use final weights from first trajectory as base\n",
    "            final_weights = trajectories[0][-1]\n",
    "            temp_mlp.W1 = final_weights['W1'].clone()\n",
    "            temp_mlp.B1 = final_weights['B1'].clone()\n",
    "            temp_mlp.W2 = final_weights['W2'].clone()\n",
    "            temp_mlp.B2 = final_weights['B2'].clone()\n",
    "\n",
    "            # Modify only W1[0,1] and W1[1,0]\n",
    "            temp_mlp.W1[0, 1] = W01_mesh[i, j]\n",
    "            temp_mlp.W1[1, 0] = W10_mesh[i, j]\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = temp_mlp.forward(torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]))\n",
    "            loss = bce_loss(Y_pred, torch.tensor([[0.], [1.], [1.], [0.]]))\n",
    "            Loss_mesh[i, j] = loss.item()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "    # Contour plot\n",
    "    contour = ax.contourf(W01_mesh, W10_mesh, Loss_mesh, levels=20, cmap='viridis', alpha=0.7)\n",
    "    ax.contour(W01_mesh, W10_mesh, Loss_mesh, levels=20, colors='black', alpha=0.2, linewidths=0.5)\n",
    "\n",
    "    # Plot all trajectories\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "    for idx, (w01_traj, w10_traj, label) in enumerate(zip(all_w01_trajs, all_w10_trajs, labels)):\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.plot(w01_traj, w10_traj, '.-', color=color, linewidth=2, markersize=3,\n",
    "                label=f'{label} Trajectory', alpha=0.8)\n",
    "        ax.plot(w01_traj[-1], w10_traj[-1], '*', color=color, markersize=15,\n",
    "                label=f'{label} End')\n",
    "\n",
    "    # Mark initial point (same for all)\n",
    "    ax.plot(all_w01_trajs[0][0], all_w10_trajs[0][0], 'ko', markersize=12,\n",
    "            label='Initial Point', zorder=10)\n",
    "\n",
    "    # Labels and legend\n",
    "    ax.set_xlabel('W1[0, 1]', fontsize=12)\n",
    "    ax.set_ylabel('W1[1, 0]', fontsize=12)\n",
    "    ax.set_title('Loss Landscape and Training Trajectories', fontsize=14)\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(contour, ax=ax)\n",
    "    cbar.set_label('Loss', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySM_x0t0rVtY"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmEkH99crTc-"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# TODO: Define XOR dataset\n",
    "#\n",
    "# Create X_train as a (4, 2) tensor with XOR inputs\n",
    "#\n",
    "# Create Y_train as a (4,) tensor with XOR targets\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# END TODO\n",
    "############################################################################\n",
    "\n",
    "# Assertions to check XOR dataset\n",
    "assert X_train is not None, \"X_train not defined\"\n",
    "assert Y_train is not None, \"Y_train not defined\"\n",
    "assert X_train.shape == (4, 2), f\"X_train shape should be (4, 2), got {X_train.shape}\"\n",
    "assert Y_train.shape == (4,), f\"Y_train shape should be (4,), got {Y_train.shape}\"\n",
    "print(\"✓ XOR dataset correctly defined\\n\")\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# TODO: Initialize weights\n",
    "#\n",
    "# Create initial_weights dictionary with given weights:\n",
    "# - 'W1': (2, 2) tensor\n",
    "# - 'B1': (2,) tensor\n",
    "# - 'W2': (2, 1) tensor\n",
    "# - 'B2': (1,) tensor\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# END TODO\n",
    "############################################################################\n",
    "\n",
    "# Assertions to check initial weights\n",
    "assert initial_weights is not None, \"initial_weights not defined\"\n",
    "assert 'W1' in initial_weights and initial_weights['W1'].shape == (2, 2), \"W1 shape should be (2, 2)\"\n",
    "assert 'B1' in initial_weights and initial_weights['B1'].shape == (2,), \"B1 shape should be (2,)\"\n",
    "assert 'W2' in initial_weights and initial_weights['W2'].shape == (2, 1), \"W2 shape should be (2, 1)\"\n",
    "assert 'B2' in initial_weights and initial_weights['B2'].shape == (1,), \"B2 shape should be (1,)\"\n",
    "print(\"✓ Initial weights correctly defined\")\n",
    "\n",
    "print(\"\\nInitial Weights:\")\n",
    "print(f\"W1:\\n{initial_weights['W1']}\")\n",
    "print(f\"B1: {initial_weights['B1']}\")\n",
    "print(f\"W2:\\n{initial_weights['W2']}\")\n",
    "print(f\"B2: {initial_weights['B2']}\\n\")\n",
    "\n",
    "# Train with SGD\n",
    "print(\"Training with SGD...\")\n",
    "mlp_sgd = MLP(initial_weights)\n",
    "####################################################################\n",
    "# TODO: Set appropriate learning rate and number of epochs below\n",
    "####################################################################\n",
    "history_sgd = train(mlp_sgd, X_train, Y_train, optimizer='sgd', lr=0, epochs=0)\n",
    "####################################################################\n",
    "# END TODO\n",
    "####################################################################\n",
    "print(f\"Final loss (SGD): {history_sgd['loss'][-1]:.4f}\")\n",
    "\n",
    "# Test predictions\n",
    "Y_pred_sgd = mlp_sgd.forward(X_train)\n",
    "print(\"\\nPredictions after SGD:\")\n",
    "for i, (x, y_true, y_pred) in enumerate(zip(X_train, Y_train, Y_pred_sgd)):\n",
    "    print(f\"  Input: {x.numpy()}, True: {y_true:.0f}, Pred: {y_pred.item():.4f}\")\n",
    "\n",
    "# Train with Adam\n",
    "print(\"\\nTraining with Adam...\")\n",
    "mlp_adam = MLP(initial_weights)\n",
    "####################################################################\n",
    "# TODO: Set appropriate learning rate and number of epochs below\n",
    "####################################################################\n",
    "history_adam = train(mlp_adam, X_train, Y_train, optimizer='adam', lr=0, epochs=0)\n",
    "####################################################################\n",
    "# END TODO\n",
    "####################################################################\n",
    "print(f\"Final loss (Adam): {history_adam['loss'][-1]:.4f}\")\n",
    "\n",
    "# Test predictions\n",
    "Y_pred_adam = mlp_adam.forward(X_train)\n",
    "print(\"\\nPredictions after Adam:\")\n",
    "for i, (x, y_true, y_pred) in enumerate(zip(X_train, Y_train, Y_pred_adam)):\n",
    "    print(f\"  Input: {x.numpy()}, True: {y_true:.0f}, Pred: {y_pred.item():.4f}\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "weight_traj_sgd = [{'W1': history_sgd['W1'][i],\n",
    "                    'B1': history_sgd['B1'][i],\n",
    "                    'W2': history_sgd['W2'][i],\n",
    "                    'B2': history_sgd['B2'][i]}\n",
    "                    for i in range(len(history_sgd['loss']))]\n",
    "\n",
    "weight_traj_adam = [{'W1': history_adam['W1'][i],\n",
    "                      'B1': history_adam['B1'][i],\n",
    "                      'W2': history_adam['W2'][i],\n",
    "                      'B2': history_adam['B2'][i]}\n",
    "                    for i in range(len(history_adam['loss']))]\n",
    "\n",
    "# Plot loss landscape with both trajectories\n",
    "print(\"\\nPlotting loss landscape...\")\n",
    "plot_loss_landscape(\n",
    "    mlp_sgd,\n",
    "    trajectories=[weight_traj_sgd, weight_traj_adam],\n",
    "    loss_trajectories=[history_sgd['loss'], history_adam['loss']],\n",
    "    labels=['SGD', 'Adam']\n",
    ")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.plot(history_sgd['loss'], label='SGD', color='red', linewidth=2)\n",
    "plt.plot(history_adam['loss'], label='Adam', color='blue', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Comparison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP8QUULvDUXzgjBgdwNrXdP",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1-mhOGYT89MfMSLBIxHMhRH4f-j2u-V26",
     "timestamp": 1759409929790
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python3 (VS Code)",
   "language": "python",
   "name": "python3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
